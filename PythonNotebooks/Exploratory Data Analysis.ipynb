{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <br>\n",
    "\n",
    "Author: Jamie McQuire <br>\n",
    "\n",
    "* This is a supplementary exploratory data analysis for the project.\n",
    "* The data used in this report is from Google's speech command dataset.\n",
    "* [Link to the challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/overview/description)\n",
    "* This report will show how the audio data can be represented in different ways, including the log-spectrogram transformation of the data which is used to train the machine learning algorithms.\n",
    "* Note: This file was created to supplement the report and the actual pre-processing files were used to generate the data are included in the submission folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import gauss\n",
    "from scipy.io import wavfile\n",
    "import csv\n",
    "from scipy import signal\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the working directory to the Data directory\n",
    "os.chdir(\"C:\\\\Users\\\\b9027741\\\\OneDrive - Newcastle University\\\\Masters\\\\Computer Science\\\\Machine_Learning_Project\\\\Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Audio Files <br>\n",
    "\n",
    "* This section of the report will read the audio files into the Python environment as time series vectors.\n",
    "* The time series vectors will then be plotted as a signal.\n",
    "* We can see that the different audio files have distinct shapes with the actual speech command occuring at different points in time.\n",
    "* The silence audio file appears to represent a stationary process which is what we would expect with noise.\n",
    "* We are not going to work with raw time series data and instead use a Discrete Fourier Transform (DFT) to plot the data as a spectrogram for image recognition.\n",
    "* The image recognition networks could be used on the signal plots, however, more information could be extracted from the different frequency components present in the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#picked 12 files to represent the full categories specified in the challenge\n",
    "audio_list = [\"yes/f2a90886_nohash_0.wav\",\"no/e32ff49d_nohash_1.wav\",\"up/d8ee4734_nohash_1.wav\",\"down/cb8f8307_nohash_6.wav\",\n",
    "              \"left/c1eebc0b_nohash_1.wav\",\"right/b93528e3_nohash_0.wav\", \"on/a8688b67_nohash_0.wav\", \"off/96ab6565_nohash_2.wav\",\n",
    "             \"stop/8eb4a1bf_nohash_0.wav\",\"go/7cf14c54_nohash_0.wav\",\"silence/noisy2_part57_pink_noise.wav\",\"six/0b09edd3_nohash_2.wav\"]\n",
    "\n",
    "#create the labels for the audio_list\n",
    "audio_labels = []\n",
    "for x in audio_list:\n",
    "    file_name = x.rsplit(\"/\")[0]\n",
    "    if file_name == \"six\":\n",
    "        file_name = \"unknown (six)\"\n",
    "    audio_labels.append(file_name)\n",
    "    \n",
    "fig,ax = plt.subplots(3,4,figsize=(25,25))\n",
    "\n",
    "for i, audio in enumerate(audio_list):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    ax= plt.gca()\n",
    "    ax.set_title(\"Audio File - \" + audio_labels[i])\n",
    "    sample_rate, audio = wavfile.read(\"train/audio/\" + audio)\n",
    "    ax.plot(audio)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Fourier Transform <br>\n",
    "\n",
    "* This section of the report is going to translate the time series audio data into a representation of its frequency components.\n",
    "* This will be achieved using a Discrete Fourier Transform (DFT) computed using the Fast Fourier Transform (FFT) algorithm.\n",
    "* The DFT representation is going to be computed for the \"yes\", \"no\", and \"up\" voice commands, along with an audio file that contains silence.\n",
    "* We can see that the audio files have frequency components with different amplitudes.\n",
    "* This means that information about the frequencies could be used to differentitate between the different voice commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the files and the audio labels\n",
    "audio_list = [\"yes/f2a90886_nohash_0.wav\",\"no/e32ff49d_nohash_1.wav\",\"up/d8ee4734_nohash_1.wav\",\"silence/noisy2_part57_pink_noise.wav\"]\n",
    "audio_labels = [\"yes\",\"no\",\"up\",\"silence\"]\n",
    "\n",
    "#fft function\n",
    "def custom_fft(y, fs): #[1]\n",
    "    T = 1.0 / fs\n",
    "    N = y.shape[0]\n",
    "    yf = scipy.fftpack.fft(y)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    vals = 2.0/N * np.abs(yf[0:N//2]) \n",
    "    return xf, vals\n",
    "\n",
    "fig,ax = plt.subplots(4,1,figsize=(25,25))\n",
    "\n",
    "#plot the ffts\n",
    "for i, audio in enumerate(audio_list):\n",
    "    plt.subplot(4, 1, i+1)\n",
    "    ax= plt.gca()\n",
    "    ax.set_title(\"FFT of audio file - \" + audio_labels[i] + \" (sampled at 16000 kHz)\")\n",
    "    sample_rate, audio = wavfile.read(\"train/audio/\" + audio)\n",
    "    xf, vals = custom_fft(audio, sample_rate)\n",
    "    ax.plot(xf, vals)\n",
    "    ax.grid()\n",
    "    ax.set_xlabel(\"Frequency (Hz)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Spectrograms\n",
    "\n",
    "* This section of the report is going to represent the DFT transformation as a log spectrogram.\n",
    "* The log spectrogram will produce a 2D image that can be used for image recognition.\n",
    "* A log spectrogram is a representation of the frequency spectrum in a time varying signal.\n",
    "* The colour of a pixel represents the power (dB) of the frequency component at the instant in time.\n",
    "* We can see from the spectrograms that each word has a distinct pattern with the shape of the yellow colours.\n",
    "* Image recognition algorithms could be used to detect this pattern and classify the voice commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the log spectrogram [1]\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)\n",
    "\n",
    "#list the files and the audio labels\n",
    "audio_list = [\"yes/f2a90886_nohash_0.wav\",\"no/e32ff49d_nohash_1.wav\",\"up/d8ee4734_nohash_1.wav\",\"silence/noisy2_part57_pink_noise.wav\"]\n",
    "audio_labels = [\"yes\",\"no\",\"up\",\"silence\"]\n",
    "\n",
    "#plot the spectrograms\n",
    "\n",
    "fig,ax = plt.subplots(2,2,figsize=(10,10))\n",
    "\n",
    "for i, audio in enumerate(audio_list):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Log Spectrogram of audio file - \" + audio_labels[i])\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    sample_rate, samples = wavfile.read(\"train/audio/\" + audio)\n",
    "    freqs, times, spec = log_specgram(audio=samples, sample_rate=16000)\n",
    "    im = ax.imshow(spec, aspect='auto', origin='lower',\n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "    fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can further investigate this by observing the spectrograms of 4 \"yes\" files and 4 \"no\" files.\n",
    "* The shape of each voice command appears to be roughly the same but being shifted up and down the x-axis.\n",
    "* This means that a CNN should be able to classify the word correctly as the CNN is resillient to translations of a pattern in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the yes files and the no files\n",
    "\n",
    "yes_files = [\"yes/0bd689d7_nohash_0.wav\",\"yes/a5d485dc_nohash_2.wav\",\"yes/a60a09cf_nohash_1.wav\",\"yes/e14a99a5_nohash_1.wav\"]\n",
    "no_files = [\"no/0bde966a_nohash_2.wav\",\"no/951cac20_nohash_1.wav\",\"no/afb9e62e_nohash_1.wav\",\"no/dc6e9c04_nohash_1.wav\"]\n",
    "\n",
    "fig,ax = plt.subplots(1,4,figsize=(50,10))\n",
    "\n",
    "for i, audio in enumerate(yes_files):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Log Spectrogram of audio file - yes\" +\"(\"+str(i)+\")\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    sample_rate, samples = wavfile.read(\"train/audio/\" + audio)\n",
    "    freqs, times, spec = log_specgram(audio=samples, sample_rate=16000)\n",
    "    im = ax.imshow(spec, aspect='auto', origin='lower',\n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    \n",
    "fig,ax = plt.subplots(1,4,figsize=(50,10))\n",
    "\n",
    "for i, audio in enumerate(no_files):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Log Spectrogram of audio file - no\" +\" (\"+str(i)+\")\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    sample_rate, samples = wavfile.read(\"train/audio/\" + audio)\n",
    "    freqs, times, spec = log_specgram(audio=samples, sample_rate=16000)\n",
    "    im = ax.imshow(spec, aspect='auto', origin='lower',\n",
    "           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "    fig.colorbar(im, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Included as instructions for how to represent the data in this form was taken from this notebook.\n",
    "\n",
    "[1] - https://www.kaggle.com/davids1992/speech-representation-and-data-exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
